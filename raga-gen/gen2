#!/usr/bin/env python3

from os import walk
import pathlib
import argparse
import json
import numpy as np
import os
import time
import torch
import transformers
import vllm
from utils import *


"""
The code is adopted based on code to invoke vllm.LLM 
shared by Nelson Mimura Gonzalez
with improvements listed on README.md file, e.g. load-balancing,
checkpoints and recover, better output files for performance

We launch N pods, each handle a portition of the same file
   portion = num_rows --> once done: mark it as done and move to new run

"""


class StatusFile:
    def __init__(self, id: str, num_workers: int, fname="/results/status.json"):
        self.fname = fname
        self.id = id  # pod-id 1-based index
        self.data = self.read()
        # read worker-specific file
        self.data.update(self.read(self.local_fname))
        self.num_workers = num_workers
        self.num_lines = 100

    @property
    def local_fname(self):
        "status file in the worker scope"
        return f"{self.fname}_{self.id}"

    def save(self, data=None):
        fname = self.local_fname
        if data is None:
            data = self.data
        with open(fname, "w") as f:
            json.dump(data, f)

    def files2process_cycle(self):
        """return an infinite iterable of files to process"""
        metadata = self.data["metadata"]
        keep_processing = True
        while keep_processing:
            keep_processing = False
            for fname in metadata.keys():
                if not fname in self.data["completed"]:
                    keep_processing = True
                    yield fname

    def files2process(self):
        """return an iterable of files to process"""
        metadata = self.data["metadata"]
        for fname in metadata.keys():
            if not fname in self.data["completed"]:
                yield fname

    def update_metadata(self, mdict):
        """a dictionary of some metadata"""
        if not self.data["metadata"]:
            self.data["metadata"].update(mdict)
        else:
            self.data["metadata"] = mdict

    def configure_run(self, last_run, **kwargs):
        self.data["run"][last_run] = {
            # each successful run process these many lines
            # "num_lines": self.num_lines,
            "num_workers": self.num_workers,
        }
        self.data["run"][last_run].update(kwargs)

    def get_data(self, fname, prompt_only=False):
        num_workers = self.num_workers
        worker_id = self.id
        num_rows = self.num_lines
        # num_rows = self.data["metadata"][fname] // num_workers
        # TUAN: ASSUMPTION - one-based pod-id

        last_run = 0
        next_start_row = 1
        if self.data["run"]:
            last_run = list(self.data["run"].keys())[-1]
            if self.data["run"][last_run]["done"]:
                next_start_row = int(self.data["run"][last_run]["next_start_row"])
                last_run = int(last_run) + 1
            else:
                # rerun it
                start_row = int(self.data["run"][last_run]["start_row"])
                num_rows = int(self.data["run"][last_run]["num_rows"])
                lines = read_prompt(
                    fname,
                    start_row,
                    start_row + num_rows - 1,
                    keep_all=(prompt_only is False),
                )
                self.fake_fname = f"{fname}__{start_row}_{len(lines)}"
                self.data["run"][last_run].update(
                    {
                        "done": True,  # place-holder to be used for saved later
                    }
                )
                return lines
        start_row = (int(worker_id) - 1) * num_rows + next_start_row  # 1-based index
        mdict = {
            "file_name": fname,
            "start_row": start_row,
            "num_rows": num_rows,
            "next_start_row": num_workers * num_rows + next_start_row,
            "done": False,
        }
        # self.fake_fname = f"{fname}__{start_row}_{num_rows}"
        self.last_run = last_run
        self.configure_run(last_run, **mdict)
        self.save()
        self.data["run"][last_run].update(
            {
                "done": True,  # place-holder to be used for saved later
            }
        )
        lines = read_prompt(
            fname, start_row, start_row + num_rows - 1, keep_all=(prompt_only is False)
        )
        self.fake_fname = f"{fname}__{start_row}_{len(lines)}"
        return lines

    def read(self, fname=None):
        if fname is None:
            fname = self.fname
        try:
            with open(fname) as f:
                mdict = json.load(f)
        except FileNotFoundError:
            mdict = {
                "run": {},
                "metadata": {},  # line count for each file
                "completed": [],  # list of completed files
            }
        # and read from all component files for global-data
        path = str(pathlib.Path(fname).parent)
        files = [
            filename
            for filename in os.listdir(path)
            if filename.startswith("status.json_")
        ]
        for fname in files:
            try:
                with open(fname) as f:
                    # FIXME assuming one pod completed may not be accurate
                    # there can be a crash on the other pod - so it needs to read this files to process its part
                    mdict["completed"].update(json.load(f)["completed"])
            except (FileNotFoundError, KeyError):
                pass

        self.data = mdict
        return mdict


def main():
    banner("parameters")

    single_file = False
    parser = get_parser(single_file)
    parser.parse_args(namespace=par)

    if not os.path.exists(par.output):
        os.makedirs(par.output)

    get_vllm(par, var)

    var.prompts = []
    if single_file:
        for line in open(par.file):
            var.prompts.append(json.loads(line)["prompt"])
        process_chunk()
    else:
        # read status
        import socket

        # a pod is a worker in this case
        worker_id = socket.gethostname().split("-")[-1]
        assert int(worker_id) > 0, f"ERROR: pod-id must be an non-zero integer"
        num_workers = par.num_workers
        status = StatusFile(worker_id, num_workers)
        status.update_metadata(
            get_files_and_offset(
                par.data_path,
            )
        )

        status.save()
        stop_request = False
        save_immediately = False
        if 0:
            # iterate every files
            for fname in status.files2process_cycle():
                # process this file
                lines = status.get_data(fname)
                var.prompts = lines
                if len(lines) > 0:
                    par.file = status.fake_fname
                    stop_request = process_chunk(save_immediately)
                    status.save()
                if stop_request:
                    return
                if len(lines) < status.num_lines and (
                    fname not in status.data["completed"]
                ):
                    status.data["completed"][fname] = id
                    status.save()
                    break
                if is_stop():
                    return
            if is_stop() or stop_request:
                return
        else:
            # finish one by one
            for fname in status.files2process():
                # process this file
                while True:
                    lines = status.get_data(fname)
                    var.prompts = lines
                    if len(lines) > 0:
                        par.file = status.fake_fname
                        stop_request = process_chunk(save_immediately)
                        status.save()
                    if stop_request:
                        return
                    if len(lines) < status.num_lines and (
                        fname not in status.data["completed"]
                    ):
                        status.data["completed"][fname] = id
                        status.save()
                        break
                    if is_stop():
                        return
                if is_stop() or stop_request:
                    return
        # process_chunk()


def get_files_and_offset(mypath, num_worker=11):
    """return a dict
    key = filename
    value = number of lines in the file
    """
    mdict = {}
    for dirpath, dirnames, filenames in walk(mypath):
        path = str(pathlib.Path(mypath).parent)
        dirname = dirpath[len(path) + 1 :]
        for fname in filenames:
            fname = pathlib.Path(dirpath) / fname
            num_lines = sum(1 for _ in open(fname))
            mdict[str(fname)] = num_lines
    return mdict


def process_chunk(save_immediately=True):
    var.n = len(var.prompts)
    var.iters = int(var.n / par.batch)

    if var.n % par.batch != 0:
        print("cannot divide # prompts by batch size")
        print()
        exit(1)

    print("model      =", par.model)
    print("tp         =", par.tp)
    print("batch size =", par.batch)
    print("file       =", par.file)
    print("prompts    =", var.n)
    print("iters      =", var.iters)

    banner("vllm")

    stop_request = False
    for i in range(var.iters):
        if par.start_at > i:
            continue
        if par.end_at == i:
            break
        process(i, save_immediately)
        if is_stop_immediately(i):
            # save status
            stop_request = True
            break

    if not save_immediately:
        f = open(
            "%s/%s.txt" % (par.output, os.path.basename(par.file)),
            "w",
            encoding="utf-8",
        )
        for j in range(var.n):
            if "response" not in var.prompts[j]:
                break
            f.write(json.dumps(var.prompts[j]))
            f.write("\n")
        f.close()
    banner("done")
    return stop_request


def is_stop():
    fname = "/model/stop_run_end_chunk.txt"
    try:
        with open(fname, "r") as f:
            first_line = f.readline()
        return bool(first_line)
    except FileNotFoundError:
        pass
    return False


def is_stop_immediately(n):
    fname = "/model/stop_run_immediately.txt"
    # check every 8 iterations
    if ((n >> 3) << 3) == n:
        try:
            with open(fname, "r") as f:
                first_line = f.readline()
            return bool(first_line)
        except FileNotFoundError:
            pass
    return False


def process(i, save_immediately=False):
    banner("process %d / %d" % (i + 1, var.iters))

    save_full_dict = False
    if isinstance(var.prompts[0], dict):
        # process dict
        save_full_dict = True
        prompts = [
            x["prompt"] for x in var.prompts[i * par.batch : (i + 1) * par.batch]
        ]
    else:
        prompts = var.prompts[i * par.batch : (i + 1) * par.batch]
    from utils import update_prompt

    prompts = update_prompt(prompts)

    t0 = time_get()
    outputs = var.llm.generate(prompts, var.sampling_params)
    t1 = time_get()
    dt = time_diff(t1, t0)

    reqs = 0
    toks = 0

    outs = []

    for output in outputs:
        for out in output.outputs:
            # FIXME: should be 'i+...' rather than 'i' to work with batch > 1
            if save_full_dict:
                var.prompts[i]["response"] = out.text  # .replace("\n", "\\n")
                # var.prompts[i]["tokens_count"] = len(out.token_ids)
                # var.prompts[i]["tokens"] = " ".join(list(map(str, out.token_ids)))
            if save_immediately:
                if save_full_dict:
                    f = open(
                        "%s/%s.txt" % (par.output, os.path.basename(par.file)),
                        "a",
                        encoding="utf-8",
                    )
                    f.write(json.dumps(var.prompts[i]))
                    f.write("\n")
                    f.close()
                else:
                    f = open(
                        "%s/%s.txt" % (par.output, os.path.basename(par.file)),
                        "a",
                        encoding="utf-8",
                    )
                    f.write("%-6d -- " % (len(out.token_ids)))
                    # f.write(out.text.replace("\n", "\\n"))
                    f.write(out.text)  # .replace("\n", "\\n"))
                    f.write("\n")
                    f.close()

                    f = open(
                        "%s/%s.ids" % (par.output, os.path.basename(par.file)),
                        "a",
                    )

                    f.write("%-6d -- " % (len(out.token_ids)))
                    f.write(" ".join(list(map(str, out.token_ids))))
                    f.write("\n")
                    f.close()

            reqs += 1
            toks += len(out.token_ids)

    print()
    print("dt    = %.1f" % (dt))
    print("reqs  = %d" % (reqs))
    print("req/s = %.1f" % (reqs / dt))
    print("toks  = %d" % (toks))
    print("tok/s = %.1f" % (toks / dt))


if __name__ == "__main__":
    main()
